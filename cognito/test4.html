<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Voice Command Video Interface</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.2/css/all.min.css">
    
    <style>
        /* BASE STYLES for the application container */
        html, body {
            height: 100%;
            margin: 0;
            overflow: hidden; /* Hide scrollbars */
        }
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            color: #f0f0f0; /* Light text color for contrast against video */
        }

        /* 1. SECTION/PAGE STYLES */
        #content-voicechat {
            width: 100vw;
            height: 100vh;
            display: flex;
            justify-content: center;
            align-items: center;
            position: relative;
            z-index: 1; /* Ensures content is above video */
        }

        /* 2. VIDEO BACKGROUND STYLES */
        #video-background {
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            object-fit: cover; /* Ensures video covers the entire area */
            z-index: -2; /* Puts it behind everything */
        }
        
        /* Dark overlay for better text readability */
        #content-voicechat::before {
            content: '';
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            background: rgba(0, 0, 0, 0.65); /* Dark semi-transparent layer */
            z-index: -1;
        }

        /* 3. CENTERED CONTENT CONTAINER */
        #interface-wrapper {
            display: flex;
            flex-direction: column;
            align-items: center;
            gap: 20px;
            width: 90%;
            max-width: 750px;
            padding: 30px;
            background: rgba(255, 255, 255, 0.1); /* Slightly transparent white box */
            border-radius: 12px;
            box-shadow: 0 4px 30px rgba(0, 0, 0, 0.3);
        }

        /* 4. CONTENT STYLES */
        h1 { color: #fff; font-size: 2.5em; margin-bottom: 0; }
        p { color: #ccc; font-size: 1.1em; text-align: center; max-width: 80%; }

        /* Navigation Button */
        .backbtn {
            position: absolute;
            z-index: 10;
            font-size: 2em;
            color: #fff;
            cursor: pointer;
        }

        /* Visualizer and Transcript Containers */
        #visualizer-container {
            width: 100%; height: 180px; 
            background-color: rgba(255, 255, 255, 0.95); /* Near white background for waves */
            border: 2px solid #333; 
            border-radius: 8px; overflow: hidden;
            display: flex; justify-content: center; align-items: center;
            box-sizing: border-box; 
        }

        #audio-visualizer { width: 100%; height: 100%; display: block; }

        #transcription-div {
            width: 100%; min-height: 80px;
            background-color: #f8f8f8; /* Light background for text */
            border: 1px dashed #aaa; 
            border-radius: 4px; padding: 15px; box-sizing: border-box;
            text-align: left; font-style: italic; color: #777;
        }

        .transcribed-text { color: #000; font-style: normal; font-weight: 500; }
        .error-message { color: #d9534f; font-weight: bold; font-style: normal; }

        /* System Response and Typewriter */
        .system-response {
            color: #176B87; 
            font-style: normal;
            font-weight: bold;
            display: block; 
            margin-top: 5px;
            border-top: 1px solid #ddd;
            padding-top: 5px;
        }
        
        .typing::after {
            content: '|';
            animation: blink 0.7s infinite;
        }

        @keyframes blink {
            0%, 100% { opacity: 1; }
            50% { opacity: 0; }
        }

        /* MIC Button */
        #status {
            padding: 12px 25px;
            background-color: #333; 
            color: #f0f0f0; 
            border-radius: 5px;
            font-weight: bold;
            cursor: pointer; 
            transition: background-color 0.3s ease;
            width: 80px; 
            text-align: center;
        }
        #status:hover { background-color: #555; }
        .mic-listening { background-color: #5cb85c; }
        .mic-error { background-color: #d9534f; }
    </style>
</head>
<body>

<section id="content-voicechat" class="page-content-section transition-opacity duration-300 min-h-full relative overflow-auto">
    
    <video autoplay muted loop id="video-background">
        <source src="vid/172156-846731269_smallnyh.mp4" type="video/mp4">
        Your browser does not support the video tag.
    </video>

    <button class="backbtn nav-btn flex top-[12px] left-[12px] md:top-[25px] md:left-[25px]" data-route="home">
        <i class="fa-regular fa-circle-xmark"></i>
    </button>
    
    <div id="interface-wrapper">
        <h1>Voice Command Simulator</h1>
        <p>Click **MIC** and use flexible phrasing like "Transfer 5000 to Melvin" to trigger a system response.</p>

        <div id="status">MIC</div>

        <div id="visualizer-container">
            <canvas id="audio-visualizer"></canvas>
        </div>

        <h2>Transcription & Response:</h2>
        <div id="transcription-div">
            <span id="final-text">Ready for command...</span>
            <span id="interim-text" style="color:#aaa;"></span>
            <span id="system-response"></span>
        </div>
    </div>
</section>


<script>
    // --- Setup Elements ---
    const canvas = document.getElementById('audio-visualizer');
    const canvasCtx = canvas.getContext('2d');
    const statusDiv = document.getElementById('status');
    const finalTextView = document.getElementById('final-text');
    const interimTextView = document.getElementById('interim-text');
    const systemResponseView = document.getElementById('system-response'); 

    // Initial canvas size setup
    function setCanvasDimensions() {
        // Ensure the canvas scales to the container's current dimensions
        canvas.width = canvas.parentElement.clientWidth;
        canvas.height = canvas.parentElement.clientHeight;
    }
    setCanvasDimensions(); 
    window.addEventListener('resize', setCanvasDimensions);

    // --- Global Variables ---
    let audioCtx;
    let analyser;
    let dataArray;
    let bufferLength;
    let animationFrameId;
    let mediaStream; 
    let recognitionTimeout;
    const RECOGNITION_TIMEOUT_MS = 5000;
    
    // --- Speech Recognition Variables ---
    let recognition;
    let isListening = false;
    const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;

    // --- FLEXIBLE COMMANDS DICTIONARY ---
    const COMMANDS = [
        {
            trigger: 'transfer',
            getResponse: (text) => {
                let amount = 'an unknown amount';
                let name = 'an unknown recipient';
                
                // 1. Find the amount (prioritizes digits, then checks for common spoken thousands)
                const amountMatch = text.match(/(\d{1,3}(?:,\d{3})*|\d+)/) || 
                                    text.match(/(one|two|three|four|five|six|seven|eight|nine)\s*(?:thousand)/i);
                if (amountMatch) {
                    amount = amountMatch[0].replace(/\s/g, '');
                } else {
                    return "TRANSFER: Please specify the amount you wish to transfer. E.g., 'transfer 5000...'";
                }

                // 2. Find the recipient after the word 'to'
                const toIndex = text.indexOf('to');
                if (toIndex !== -1) {
                    let nameSegment = text.substring(toIndex + 2).trim();
                    name = nameSegment.replace(/\s*(now|today|please|quickly|my|a|an)\s*$/i, '').trim();
                    if (name.length < 2) {
                         return `TRANSFER: I have the amount ${amount}, but who should I send it to?`;
                    }
                } else {
                    return `TRANSFER: I have the amount ${amount}, but I couldn't find the recipient's name.`;
                }
                
                // Final Success Response
                return `CONFIRM: Transfer ${amount} NGN to ${name.toUpperCase()}. Reply 'YES' to proceed.`;
            }
        },
        {
            trigger: 'time',
            getResponse: () => {
                return `The current time is ${new Date().toLocaleTimeString('en-US', {hour: '2-digit', minute: '2-digit', second: '2-digit', hour12: true})}.`;
            }
        },
        {
            trigger: 'call',
            getResponse: (text) => {
                const nameMatch = text.match(/call\s+([a-zA-Z\s]+)/i);
                const name = nameMatch ? nameMatch[1].trim() : 'a contact';
                
                if (name.length < 2 || name === 'a contact') {
                     return "CALL: Please state who you would like to call. E.g., 'call my wife'.";
                }
                return `Calling ${name.toUpperCase()} now. Please wait...`;
            }
        }
    ];
    
    // --- Typewriter Effect Function ---
    let typeInterval;
    function startTypewriterEffect(message) {
        clearInterval(typeInterval);
        systemResponseView.textContent = '';
        systemResponseView.classList.add('system-response');
        systemResponseView.classList.add('typing');

        let i = 0;
        typeInterval = setInterval(() => {
            if (i < message.length) {
                systemResponseView.textContent += message.charAt(i);
                i++;
            } else {
                clearInterval(typeInterval);
                systemResponseView.classList.remove('typing'); 
            }
        }, 35); 
    }

    // --- Helper Functions to manage MIC button state ---
    function setMicState(state) {
        statusDiv.classList.remove('mic-listening', 'mic-error');
        statusDiv.style.cursor = 'pointer';
        
        if (state === 'listening') {
            statusDiv.classList.add('mic-listening');
            statusDiv.style.cursor = 'default';
        } else if (state === 'error') {
            statusDiv.classList.add('mic-error');
        } else { 
            statusDiv.textContent = "MIC";
        }
    }
    
    function setTranscriptionStatus(message, isError = false) {
        finalTextView.textContent = message;
        interimTextView.textContent = '';
        if (isError) {
            finalTextView.classList.add('error-message');
        } else {
            finalTextView.classList.remove('error-message');
            systemResponseView.textContent = ''; 
            clearInterval(typeInterval); 
        }
    }
    
    // --- Command Processor ---
    function processCommand(text) {
        const commandText = text.toLowerCase().trim();
        let commandFound = false;
        
        for (const command of COMMANDS) {
            if (commandText.includes(command.trigger)) {
                const response = command.getResponse(commandText); 
                startTypewriterEffect(response);
                commandFound = true;
                break; 
            }
        }
        
        if (!commandFound && commandText.length > 5) {
            startTypewriterEffect("Command not recognized. Try saying 'transfer 5000' or 'what is the time'.");
        }
    }
    
    // --- 1. Audio Initialization (Web Audio API) ---
    function initAudio(stream) {
        if (!audioCtx) {
            audioCtx = new (window.AudioContext || window.webkitAudioContext)();
        }
        
        if (!analyser) {
            const source = audioCtx.createMediaStreamSource(stream);
            analyser = audioCtx.createAnalyser();
            
            analyser.fftSize = 2048;
            bufferLength = analyser.frequencyBinCount;
            dataArray = new Uint8Array(bufferLength);

            source.connect(analyser);
            analyser.connect(audioCtx.destination); 
            
            drawVisualizer();
        }
    }

    // --- 2. Speech Recognition Initialization (Web Speech API) ---
    function initSpeechRecognition() {
        if (!SpeechRecognition) {
            setTranscriptionStatus("FATAL ERROR: Speech Recognition not supported in this browser. Use Chrome.", true);
            setMicState('error');
            return false;
        }

        if (recognition) return true; 

        recognition = new SpeechRecognition();
        recognition.continuous = true; 
        recognition.interimResults = true; 
        recognition.lang = 'en-US'; 

        recognition.onstart = () => {
            isListening = true;
            setMicState('listening');
            setTranscriptionStatus("Go ahead and speak. Listening...");
            systemResponseView.textContent = '';
            
            clearTimeout(recognitionTimeout);
            recognitionTimeout = setTimeout(() => {
                setTranscriptionStatus('Listening for speech...', false);
            }, RECOGNITION_TIMEOUT_MS);
        };

        recognition.onresult = (event) => {
            clearTimeout(recognitionTimeout);
            
            let interimTranscript = '';
            let finalTranscript = '';

            for (let i = event.resultIndex; i < event.results.length; ++i) {
                const transcript = event.results[i][0].transcript;
                if (event.results[i].isFinal) {
                    finalTranscript += transcript + ' ';
                } else {
                    interimTranscript += transcript;
                }
            }

            if (finalTranscript) {
                finalTextView.innerHTML += `<span class="transcribed-text">${finalTranscript}</span>`;
                finalTextView.classList.remove('error-message');
                
                const currentTranscribedText = finalTextView.textContent.trim();
                processCommand(currentTranscribedText); 
            }
            interimTextView.textContent = interimTranscript;

            recognitionTimeout = setTimeout(() => {
                setTranscriptionStatus('Listening for speech...', false);
            }, RECOGNITION_TIMEOUT_MS);
        };

        recognition.onerror = (event) => {
            setTranscriptionStatus(`Speech Recognition Error (${event.error}). Click MIC to retry.`, true);
            setMicState('error');
            isListening = false;
            if (recognition) recognition.stop();
            if (audioCtx) audioCtx.suspend();
            clearTimeout(recognitionTimeout);
        };

        recognition.onend = () => {
            if (isListening) {
               recognition.start(); 
            } else {
                setMicState('ready'); 
            }
        };

        return true;
    }

    // --- 3. Visualization Drawing Loop ---
    function drawVisualizer() {
        animationFrameId = requestAnimationFrame(drawVisualizer);
        analyser.getByteTimeDomainData(dataArray);

        // Ensure canvas background remains transparent (or very light) for video contrast
        canvasCtx.fillStyle = 'rgba(255, 255, 255, 0.95)'; 
        canvasCtx.fillRect(0, 0, canvas.width, canvas.height); 

        canvasCtx.lineWidth = 2;
        canvasCtx.strokeStyle = 'rgb(0, 0, 0)'; 

        canvasCtx.beginPath();
        const sliceWidth = canvas.width * 1.0 / bufferLength;
        let x = 0;

        for(let i = 0; i < bufferLength; i++) {
            const v = dataArray[i] / 128.0; 
            const y = v * canvas.height / 2; 

            if(i === 0) {
                canvasCtx.moveTo(x, y); 
            } else {
                canvasCtx.lineTo(x, y); 
            }
            x += sliceWidth;
        }

        canvasCtx.lineTo(canvas.width, canvas.height/2); 
        canvasCtx.stroke(); 
    }

    // --- 4. Unified Start Handler ---
    function startAllFeatures() {
        if (!initSpeechRecognition()) return;

        if (isListening) {
            isListening = false;
            if (recognition) recognition.stop();
            if (audioCtx) audioCtx.suspend();
            setMicState('ready');
            setTranscriptionStatus("Recording stopped. Click MIC to restart.");
            clearInterval(typeInterval); 
            clearTimeout(recognitionTimeout);
            return;
        }

        navigator.mediaDevices.getUserMedia({ audio: true })
            .then(stream => {
                mediaStream = stream; 
                if (audioCtx && audioCtx.state === 'suspended') {
                    audioCtx.resume();
                } else if (!audioCtx) {
                    initAudio(stream);
                }
                
                if (!isListening) {
                    finalTextView.innerHTML = '';
                    interimTextView.textContent = '';
                    recognition.start();
                }
            })
            .catch(err => {
                setTranscriptionStatus(`Microphone Denied/Failed: ${err.name}. Check browser permissions.`, true);
                setMicState('error');
                console.error('Microphone access error:', err);
            });
    }

    // --- 5. User Interaction Listener (for activation) ---
    statusDiv.addEventListener('click', startAllFeatures);
</script>

</body>
</html>